{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IbrahemAmar/Data-mining-and-Machine-Learning-/blob/main/Facial_Emotion_Recognition_using_CNN_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztky6L9E-lnP"
      },
      "source": [
        "# **Facial Emotion Recognition**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Dataset - https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHrzqEejOjR9"
      },
      "source": [
        "_author_ = \" Prudhvi_GNV \""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CULEspsFALh"
      },
      "source": [
        "**kaggle --account--Api**\n",
        "API's are used to access data from kaggle\n",
        "1. kaggle a/c -> my profile -> create api token\n",
        "2. In colabs ..while uploading api file... go to /content\n",
        "3. make .kaggle directory in /root\n",
        "4. copy kaggle api from /content to /root/.kaggle\n",
        "5. kaggle cmd(find in kaggle dataset page-> data) to download\n",
        "6. unzip the downloaded dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbi7hj_R8_yD"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt4jf1OW_6gf"
      },
      "source": [
        "cd /root"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEY-3HG5AHFB"
      },
      "source": [
        "mkdir .kaggle               # .kaggle not /.kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNO-HFFq85a0"
      },
      "source": [
        "cd /root/.kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqSXEsJ29HP7"
      },
      "source": [
        "!cp /content/kaggle.json ~/.kaggle\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bN48VBLQpQb3"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YSkULw_pT0c"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Vlkz9IP9otV"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEkxCltOmE5v"
      },
      "source": [
        "# if ..looks like outdated api ..server version ERROR occurs\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytfZi3nJmilT"
      },
      "source": [
        "#if ..only read permisions ..ERROR\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTPl804d77nk"
      },
      "source": [
        "# if 403 forbidden ERROR ... then accept the rules in kaggle dataset page\n",
        "\n",
        "!kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grCSTKE-_UG3"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEobtx6O9qvT"
      },
      "source": [
        "!unzip challenges-in-representation-learning-facial-expression-recognition-challenge.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAiYFWW7_vMT"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQgMssSBwypP"
      },
      "source": [
        "!tar -xf fer2013.tar.gz   # x= extract , f= to create archive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiO3ojiRw4wW"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIq55w1FF5fQ"
      },
      "source": [
        "cd fer2013"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaVtewzUF-zA"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YeU4HHZ_bS4"
      },
      "source": [
        "## **Dataset**\n",
        "* The data consists of 48x48 pixel grayscale images of faces.\n",
        "\n",
        "* The task is to categorize each face based on the emotion shown in the facial expression in to one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral).\n",
        "* Dataset contains two columns, \"emotion\" and \"pixels\".\n",
        "* The \"emotion\" column contains a numeric code ranging from 0 to 6, inclusive, for the emotion that is present in the image.\n",
        "* The \"pixels\" column contains a string surrounded in quotes for each image. The contents of this string a space-separated pixel values in row major order. test.csv contains only the \"pixels\" column and your task is to predict the emotion column."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Zu89RTzBMty"
      },
      "source": [
        "# Data Visualization\n",
        "\n",
        "* powerful visualization tools => matplotlib, seaborn\n",
        "      1. matplotlib works on pandas dataframe\n",
        "      2.  matplotlib => basic ploting =>bars,pie,scatter, lines etc.. => used for MATLAB like graphs\n",
        "      3. seaborn => statistical ploting => depends on matplotlib...high level => provides default templates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcfiDv03-cGX"
      },
      "source": [
        "\"\"\" IMPORT ALL DEPENDENCIES\"\"\"\n",
        "\n",
        "\n",
        "# NumPy for numerical computing\n",
        "import numpy as np\n",
        "\n",
        "# Pandas for DataFrames\n",
        "import pandas as pd\n",
        "\n",
        "# Matplotlib for visualization\n",
        "from matplotlib import pyplot as plt\n",
        "# display plots in the notebook\n",
        "%matplotlib inline\n",
        "# import color maps\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# Seaborn for easier visualization\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTJESy7QA-5E"
      },
      "source": [
        "df = pd.read_csv(\"/root/.kaggle/fer2013/fer2013.csv\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbnhU1whADCT"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prR_SJxiBA8U"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k39FElpEMVju"
      },
      "source": [
        "## **Plotting emotion**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZidMumXvBzpr"
      },
      "source": [
        "\"\"\" matplotlib => to define size , sns => to use counterplot \"\"\"\n",
        "\n",
        "plt.figure(figsize=(9,4))\n",
        "sns.countplot(x='emotion', data=df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qEMmC3ILEtp"
      },
      "source": [
        "df['emotion'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wah5ra4rMB0O"
      },
      "source": [
        "* where ** 0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iblqx4zrSuUY"
      },
      "source": [
        "### Observation\n",
        "* for Digust we have about 547 images only very less."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4G1tEVgMnl0"
      },
      "source": [
        "## **Plotting Usage**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL5-9TQJMAbi"
      },
      "source": [
        "plt.figure(figsize=(9,4))\n",
        "sns.countplot(x='Usage', data=df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTzE2z-2Lpsh"
      },
      "source": [
        "df['Usage'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_Xc1kb96J_I"
      },
      "source": [
        " **Intuition**\n",
        "\n",
        "* % matplotlib inline ==> magic cmd to setup IPython to display and store figures in notebooks\n",
        "* dataframe ==>pd.read_csv() , .head(), .tail(), .shape\n",
        "* count ==>  sns.countplot(x=,df) -> for graphs  ; df[col].value_counts() -> to display count values\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLpNo_Wi78C3"
      },
      "source": [
        "# PreProcessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OshjJPMx95JF"
      },
      "source": [
        "import cv2\n",
        "image_size=(48,48)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRs4m6UvM7AO"
      },
      "source": [
        "pixels = df['pixels'].tolist() # Converting the relevant column element into a list for each row\n",
        "width, height = 48, 48\n",
        "faces = []\n",
        "\n",
        "for pixel_sequence in pixels:\n",
        "  face = [int(pixel) for pixel in pixel_sequence.split(' ')] # Splitting the string by space character as a list\n",
        "  face = np.asarray(face).reshape(width, height) #converting the list to numpy array in size of 48*48\n",
        "  face = cv2.resize(face.astype('uint8'),image_size) #resize the image to have 48 cols (width) and 48 rows (height)\n",
        "  faces.append(face.astype('float32')) #makes the list of each images of 48*48 and their pixels in numpyarray form\n",
        "\n",
        "faces = np.asarray(faces) #converting the list into numpy array\n",
        "faces = np.expand_dims(faces, -1) #Expand the shape of an array -1=last dimension => means color space\n",
        "emotions = pd.get_dummies(df['emotion']).to_numpy() #doing the one hot encoding type on emotions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPsO_9sn77EB"
      },
      "source": [
        "print(faces[0]) #Pixels after preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2JENcOV_nXJ"
      },
      "source": [
        "print(faces.shape)\n",
        "print(faces[0].ndim)\n",
        "print(type(faces))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeQMsBoR-Y4d"
      },
      "source": [
        "print(emotions[0]) #Emotion after preprocessing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOO-ZRSf-hgt"
      },
      "source": [
        "print(emotions.shape)\n",
        "print(emotions.ndim)\n",
        "print(type(emotions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kj9OqVepeZ2E"
      },
      "source": [
        "**Intuition**\n",
        "* Pandas series(or col of dataframe) to list => tolist() =>gives list of string of values->each string is a image\n",
        "* split each string(image) in list and cvt them into list of list of integers\n",
        "* convert list into numpy array =>np.asarray(list)\n",
        "* numpy array has reshape(width,height)\n",
        "* cv2 has resize(nparray, (size)) -->cvt image into uint8(unsigned 8 bit integer) -> has range [0,255]\n",
        "  1. uint16 => [0,65535]\n",
        "  2. uint32 => 32 bit or 4 bytes => [0, 2^32]\n",
        "  3. float => [-1,1] or [0,1]\n",
        "  4. int8 => [-128, 127]\n",
        "  5. int =>  [-2^31 , 2^31 -1]\n",
        "* images => ntg but numpy array\n",
        "* openCV has 3 dimensions => width,height, color => color space = BGR ; dtype => by default uint8\n",
        "* reshape returns a copy [ doesn't change values] whereas resize changes the values[not return a copy]\n",
        "* np.expand_dims => inserts new axis => ntg but boxes(just think for convenience) => to increase dimensions => to expand shape\n",
        "  1. axis= 0 -> box to all values ; axis =1 -> boxes to each individual values\n",
        "  2. example , (2,1,2) =>[ [[[],[]]],[[[],[]]] ] => list has 2 boxes - each box has 1 box - inside that each 1 box has 2 boxes --> in general box is a list or value\n",
        "* pd.get_dummies => convert pd series into dummy coded dataframes or indicator variables. => categorical var into indicators vars. => s.no(row names) * values(cols names) - 0,1 values.\n",
        "* numpy.asmatrix(data,dtype) => interpret input as a matrix => doesn't return a copy of matrix if given input is a matrix.\n",
        "  1. In list[0][1] =>In numpy[0,1] -> means In first box 2nd ele.\n",
        "*pd.df.to_numpy() => converts pandas dataframe into numpy array -> ndarray - usally 2 dim ->\n",
        "  1. dataframe -> some kind of tabular format with some indexes. ; numpy array -> some kind of lists of lists\n",
        "  2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL78P7u0I1Hp"
      },
      "source": [
        "# Splitting the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28T5zBUCUn4j"
      },
      "source": [
        "## Scaling the pixels between -1 and 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2MJ1pnPJCyP"
      },
      "source": [
        "x = faces.astype('float32')\n",
        "x = x / 255.0 #Dividing the pixels by 255 for normalization  => range(0,1)\n",
        "\n",
        "# Scaling the pixels value in range(-1,1)\n",
        "x = x - 0.5\n",
        "x = x * 2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foWH2FX3JErZ"
      },
      "source": [
        "print(x[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_QhWHs_JGMx"
      },
      "source": [
        "type(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8F5u4B83NKFy"
      },
      "source": [
        "plt.plot(x[0,0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Euq0pES3Ufv_"
      },
      "source": [
        "print(x.min(),x.max()) # we can observe that pixels are scale"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZARK_oB_IuF"
      },
      "source": [
        "## Splitting the dataset into train & validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oIyb2zMU2sj"
      },
      "source": [
        "num_samples, num_classes = emotions.shape\n",
        "\n",
        "num_samples = len(x)\n",
        "num_train_samples = int((1 - 0.2)*num_samples)\n",
        "\n",
        "# Traning data\n",
        "train_x = x[:num_train_samples]\n",
        "train_y = emotions[:num_train_samples]\n",
        "\n",
        "# Validation data\n",
        "val_x = x[num_train_samples:]\n",
        "val_y = emotions[num_train_samples:]\n",
        "\n",
        "train_data = (train_x, train_y)\n",
        "val_data = (val_x, val_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBtesONJWvDY"
      },
      "source": [
        "print('Training Pixels',train_x.shape)  # ==> 4 dims -  no of images , width , height , color\n",
        "print('Training labels',train_y.shape)\n",
        "\n",
        "print('Validation Pixels',val_x.shape)\n",
        "print('Validation labels',val_y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3u-V2UMmKTI"
      },
      "source": [
        "* shape => returns no of samples , classes ; len => no of samples\n",
        "* training -> x = x[: 0.8*no_of_samples] , y = y[: 0.8*no_of_samples]     ---> take the benefit of slicing\n",
        "* validation -> x = x[ 0.8*no_of_samples  : ] , y = y[ 0.8*no_of_samples  : ]\n",
        "* data = (x,y)\n",
        "\n",
        "* take benefit of type(), print() --> for checking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyZvjJKTAMpx"
      },
      "source": [
        "# Defining Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKpD2mbkD3V0"
      },
      "source": [
        "#load the libaray to built the model\n",
        "from keras.layers import Activation, Convolution2D, Dropout, Conv2D\n",
        "from keras.layers import AveragePooling2D, BatchNormalization\n",
        "from keras.layers import GlobalAveragePooling2D\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import SeparableConv2D\n",
        "from keras import layers\n",
        "from keras.regularizers import l2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f_8pLj1p85S"
      },
      "source": [
        "* keras is a deep learning library => powerful easy to use library => high level API for tensorflow, Theano => wrapper\n",
        "* minimalistic , modular approach\n",
        "* obviously,oversimplification ->abstract representations of neural networks\n",
        "* import\n",
        "  1. Sequential => linear stack of NN layers -> feed forward cnn\n",
        "  2. layers -> for almost any nn\n",
        "  3. CNN layers -> convolution2D , maxpooling2D\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg7wHibYv2wF"
      },
      "source": [
        "\"\"\"\n",
        "* keras.__version__\n",
        "* pip install --upgrade keras\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyPOvVbXtSUi"
      },
      "source": [
        "* Deep learning led to advances in computer vision\n",
        "* Neural networks learn more complex features from input image.\n",
        "  1. input layer => feeding input\n",
        "  2. first hidden layer =>  only learn local edge patterns\n",
        "  3. each subsequent layer => filters => learns more complex representations\n",
        "  4. classify\n",
        "\n",
        "* CNN => reduce no of parameters that need to be tuned => handle high dimensionality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lni4RvfCDtgD"
      },
      "source": [
        "# 1) Simpler CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h71E2cTFWOi"
      },
      "source": [
        "input_shape=(48, 48, 1)\n",
        "num_classes = 7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgnHh0urDr7U"
      },
      "source": [
        "\"\"\" Building up Model Architecture \"\"\"\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Convolution2D(filters=16, kernel_size=(7, 7), padding='same',\n",
        "                            name='image_array', input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(filters=16, kernel_size=(7, 7), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
        "model.add(Dropout(.5))\n",
        "\n",
        "model.add(Convolution2D(filters=32, kernel_size=(5, 5), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(filters=32, kernel_size=(5, 5), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
        "model.add(Dropout(.5))\n",
        "\n",
        "model.add(Convolution2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
        "model.add(Dropout(.5))\n",
        "\n",
        "model.add(Convolution2D(filters=128, kernel_size=(3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(filters=128, kernel_size=(3, 3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
        "model.add(Dropout(.5))\n",
        "\n",
        "model.add(Convolution2D(filters=256, kernel_size=(3, 3), padding='same'))\n",
        "\n",
        "\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "model.add(Convolution2D(filters=num_classes, kernel_size=(3, 3), padding='same'))\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Activation('softmax',name='predictions'))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOB8szvd_tfd"
      },
      "source": [
        " # **Intuition**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "* **Convolution** => matrix multiplication with filter -> feature detector\n",
        "  * Convolution2d => for filtering windows of 2 dimensional input -> if 1st layer = input_shape\n",
        "* filters = windows ; 2 dim filter => sliding window\n",
        "* sliding window slides over each channel and summarising the features.\n",
        "*  **batch normalisation** => used to stabilise perhaps accelerate learning process => standardise layer inputs => by applying transformation that maintains mean activation close to 0 and activation standard deviation(sq root of variance ---> how far) close to 1.\n",
        "  1. normalisation => process tend to follow bell shape curve known as normal distribution\n",
        "  2. backpropagation => updated layer by layer backward from output to the input using estimate of error that assumes weights in the layer prior to the current layer are fixed.\n",
        "  3. gradient tells how to update each parameter under the assumption that other layers do not change.\n",
        "  4. all layers changes during an update --> this update procedure leads to forever chasing a moving target.\n",
        "  5. batch normalisation => technique to coordinate the update  of multiple layers in the model => reparametrization of network\n",
        "  6. its about standardize the mean and variance of each unit in normal dist\n",
        "  7. its all about standardixeing inputs to layers for each mini batch\n",
        "\n",
        "* **Activation(ReLu)**: activation layer( non linear layer)\n",
        "  1. convention is to apply after conv layer\n",
        "  2. to introduce non linearity to a system that has computed linear operations in conv\n",
        "  3. Rectified linear unit => widely used  than non linear functions(sigmoid, tanh) for its fast training with out accuracy. => max(0,x)\n",
        "  4. Relu also alleviates vanishing gradient (lower layers of network trains very slowly because the gradient decreases slowly throught layers.\n",
        "  5. without these non linear functions(activation functions) , the network would be a large linear classifier that could simplified by multiplying weight matrices(accounting for bais) . It wouldn't do anything interesting such as image classification etc..\n",
        "* **Pooling** => max, avg ,globalmax, globalavg\n",
        "  1. conv > activation > pooling\n",
        "  2.to reduce dimensions of feature map => reduces parameters to learn and amt of computations\n",
        "  3. it further summarizes the feature map  instead of precisely positioned features generated by conv layer. This makes model more robust to variations in the position of features in image\n",
        "  4.when network wants to detect higher level features from low level building blocks (detecting corners from edges) . we dont need a rigid about exact position => we need translational invariance at the feature level . so insert pooling\n",
        "  5. overcomes the problem of sensitive to the location of the features.\n",
        "  6. local translation invariance\n",
        "\n",
        "*  **Dropout** => regularization technique\n",
        "  1.neurons are randomly dropped while training\n",
        "  2. this effect makes network less sensitive to thespecific weights of neurons\n",
        "  3. better generalization - less overfit\n",
        "\n",
        "\n",
        "---\n",
        "**Functions:**\n",
        "* convolution2d => no of filters or kernels , kernal size , padding => same(zero padding) , valid (no padding) for input ; stride = step or movement of kernel. ; if 1st layer -> input_shape\n",
        "* BatchNormalisation => no args\n",
        "* Activation => func name\n",
        "* AveragePooling2D => 2D means 2 dimensional feature map ; kernel size , padding\n",
        "* Dropout => % of drop of neurons\n",
        "* AT output , convolution2d --> filters = no of classes ; activation --> softmax\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1DSW_zoN1hg"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS2a73LMMXsF"
      },
      "source": [
        "# parameters\n",
        "batch_size = 32 #Number of samples per gradient update\n",
        "num_epochs = 200 # Number of epochs to train the model.\n",
        "#input_shape = (64, 64, 1)\n",
        "verbose = 1 #per epohs  progress bar\n",
        "num_classes = 7\n",
        "patience = 50\n",
        "base_path = 'drive/Colab Notebooks/emotion/simplecnn/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZGzV6XKVkmr"
      },
      "source": [
        "# from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
        "# from keras.callbacks import ReduceLROnPlateau\n",
        "# from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIB613fzVrlK"
      },
      "source": [
        "## Data Augmenttion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0vSiW0aVWoi"
      },
      "source": [
        "\"\"\" Data Augmentation => taking the batch and apply some series of random transformations (random rotation, resizing, shearing)\n",
        "\n",
        "      ===> to increase generalizability of model  \"\"\"\n",
        "\n",
        "\n",
        "# data generator Generate batches of tensor image data with real-time data augmentation\n",
        "data_generator = ImageDataGenerator(\n",
        "                        featurewise_center=False,\n",
        "                        featurewise_std_normalization=False,\n",
        "                        rotation_range=10,\n",
        "                        width_shift_range=0.1,\n",
        "                        height_shift_range=0.1,\n",
        "                        zoom_range=.1,\n",
        "                        horizontal_flip=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIW7y7yWkwJ3"
      },
      "source": [
        "**ImageDataGenerator**\n",
        "* horizontal and vertical shift => moving all pixels of image in one direction\n",
        "  1. width_shift_range ( horizonatal shift)\n",
        "  2. height_shift_range (vertical shift)\n",
        "  3. floating num [0- 1] --> % of shift\n",
        "\n",
        "* horizontal n vertical flips augmentation ==>reversing rows or cols of pixels  --> True or False\n",
        "* Random rotation  --> 0 - 360 degrees --> rotation_range = 90 ==> means random rotation to image blw 0 and 90 degrees\n",
        "* random  brightness --> randomly darkens or brightens images ==> brightness_range =[0.2,1.0] --> means darkens or brightens if pixel is blw 0.2 and 1\n",
        "* random zoom\n",
        "  1. either adds pixel or subtract pixels in image . [1-value, 1+value]\n",
        "  2. for example , zoom_range = .3 --> range [0.7, 1.3] or blw 70%(zoom in) and 130% (zoomout)\n",
        "\n",
        "---\n",
        "when an object is created using following args. an iterator can be created for an image dataset.\n",
        "* it iterates through all images in memory --> obj.flow(X,y)\n",
        "* to iterates images through subdirectories --> obj. flow_from_directory(X,y,..)\n",
        "* to train ==> fit_generator()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeF9vG6oWMB6"
      },
      "source": [
        "# model parameters/compilation\n",
        "\n",
        "\"\"\" CONFIGURATION ==>.compile(optimizer, loss , metrics) \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZP8riBlO4mS"
      },
      "source": [
        "import os\n",
        "\n",
        "datasets = ['fer2013']\n",
        "num_epochs = 30\n",
        "base_path = \"/content\"\n",
        "patience = 5  # Ensure this is defined\n",
        "\n",
        "for dataset_name in datasets:\n",
        "    print('Training dataset:', dataset_name)\n",
        "\n",
        "    # callbacks\n",
        "    log_file_path = dataset_name + '_emotion_training.log'\n",
        "\n",
        "    csv_logger = CSVLogger(log_file_path, append=False)\n",
        "    early_stop = EarlyStopping('val_loss', patience=patience)\n",
        "    reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience/4), verbose=1)\n",
        "\n",
        "    # FIX 1: Add '/' separators so the path is readable\n",
        "    trained_models_path = base_path + '/' + dataset_name + '_simple_cnn'\n",
        "\n",
        "    # FIX 2: Change extension from .hdf5 to .keras\n",
        "    model_names = trained_models_path + '.{epoch:02d}-{val_loss:.2f}.keras'\n",
        "\n",
        "    model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1, save_best_only=True)\n",
        "    my_callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]\n",
        "\n",
        "    # loading dataset\n",
        "    train_faces, train_emotions = train_data\n",
        "\n",
        "    # FIX 3: model.fit_generator is deprecated; switched to model.fit which handles generators automatically\n",
        "    history = model.fit(\n",
        "        data_generator.flow(train_faces, train_emotions, batch_size),\n",
        "        epochs=num_epochs,\n",
        "        verbose=1,\n",
        "        callbacks=my_callbacks,\n",
        "        validation_data=val_data\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py6W0E8z6SId"
      },
      "source": [
        "**keras**\n",
        "* **callbacks** => an object that can perform actions at various stages of training\n",
        "      1. write tensorflowboard logs after every batch\n",
        "      2. periodically save model to disk\n",
        "      3. do early stopping\n",
        "      4. view on internal states and statistics during training\n",
        "      * used in fit() loop\n",
        "  * **CSVLogger(filename, separator=\",')**  --> to save epoch results to a csv file\n",
        "      * create obj and use that obj in fit(callbacks=[csv_logger_obj])\n",
        "  * **EarlyStopping()** --> stop training when a monitored metric\n",
        "  has stopped improving\n",
        "      1. monitor = \"val_loss\" --> loss function to be monitored\n",
        "      2. min_delta --> minimum change to count(threshold)\n",
        "      3. patience --> no of epochs with no improvement to stop training\n",
        "  * **ReduceLROnPlateau()**--> reduce learning rate when metric has stopped improving\n",
        "      1. monitor, patience, min_delta\n",
        "      2. factor = 0.1 ==> learning rate reduced to 10% (lr*0.1)\n",
        "      3. verbose ==> 0: quiet , 1: update msgs\n",
        "  * **ModelCheckpoint()** -->to save  keras model or model weights at some frequency\n",
        "      1. filepath\n",
        "      2. monitor --> val_acc or val_loss\n",
        "      3. save_best_only = True\n",
        "\n",
        "---\n",
        "fit_generator(.flow(X,y, batchsize), verbose,epochs,validation_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aTQpoDjDdsU"
      },
      "source": [
        "* fit() ==> training loop\n",
        "* logs ==> dict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vI4nPHW1WZ4r"
      },
      "source": [
        "#evaluate() returns [loss,acc]\n",
        "score = model.evaluate(val_x, val_y, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1]*100)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXedkSpQncmu"
      },
      "source": [
        "**history** ====> default callbacks that is registered when training\n",
        "\n",
        "        1.  records training metrics for each epoch\n",
        "        2. these metrics stored in dictionary in history member of object returned\n",
        "        3. obj returns from calls to fit() used to train model\n",
        "        4. data collected in history obj  used to create plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzP_WR5DqZJE"
      },
      "source": [
        "\"\"\" metrics collected by history object \"\"\"\n",
        "history_dict=history.history\n",
        "history_dict.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxGe4TkroeLK"
      },
      "source": [
        "print(history_dict[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzqA8Zq-UyqL"
      },
      "source": [
        "\"\"\" Visualising model training history \"\"\"\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(history_dict['accuracy']) + 1)\n",
        "\n",
        "plt.plot(epochs, train_loss_values, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEe3qvhv5ZA9"
      },
      "source": [
        "train_acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "plt.plot(epochs, train_acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qN9xk7ThKwQ"
      },
      "source": [
        "## Testing the model on  some images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## load model\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "\n",
        "# Path to the specific file you uploaded\n",
        "model_path = 'fer2013_simple_cnn.21-1.16.keras'\n",
        "\n",
        "print(f\"Loading model from {model_path}...\")\n",
        "\n",
        "try:\n",
        "    # Load the model\n",
        "    model = load_model(model_path)\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "    # Verify the architecture\n",
        "    model.summary()\n",
        "\n",
        "    # Optional: Quick test to make sure it predicts correctly\n",
        "    # Creating a blank image matching your input shape (48x48, grayscale)\n",
        "    dummy_input = np.zeros((1, 48, 48, 1))\n",
        "    prediction = model.predict(dummy_input)\n",
        "    print(\"Test prediction successful.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")"
      ],
      "metadata": {
        "id": "R0luXhe_xXux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k1XVnmqbCNu"
      },
      "source": [
        " emotion_dict = {0: \"Neutral\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Sad\", 5: \"Surprised\", 6: \"Neutral\"}\n",
        "\n",
        "  #emojis unicodes #\n",
        " emojis = { 0:\"\\U0001f620\",1:\"\\U0001f922\" ,2:\"\\U0001f628\" ,3:\"\\U0001f60A\" , 4:\"\\U0001f625\" ,5:\"\\U0001f632\",6:\"\\U0001f610\" }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9Tm7hlE7953"
      },
      "source": [
        "print(emojis.values(),sep=\" \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC8haBpdsQnR"
      },
      "source": [
        "!cd content\n",
        "!pwd\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def _predict(path):\n",
        "  # --- THE FIX IS HERE ---\n",
        "  # We use cv2.data.haarcascades + filename to find the built-in file\n",
        "  facecasc = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "  # Safety check\n",
        "  if facecasc.empty():\n",
        "    print(\"Error: Could not load face detector!\")\n",
        "    return\n",
        "\n",
        "  imagePath = '/content/' + path\n",
        "  image = cv2.imread(imagePath)\n",
        "\n",
        "  if image is None:\n",
        "    print(f\"Error: Could not load image at {imagePath}\")\n",
        "    return\n",
        "\n",
        "  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "  faces = facecasc.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=10)\n",
        "  print(\"No of faces : \", len(faces))\n",
        "\n",
        "  i = 1\n",
        "  for (x, y, w, h) in faces:\n",
        "    cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "    roi_gray = gray[y:y + h, x:x + w]\n",
        "\n",
        "    # Resize to 48x48 for the model\n",
        "    cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
        "\n",
        "    # Predict\n",
        "    prediction = model.predict(cropped_img)\n",
        "    maxindex = int(np.argmax(prediction))\n",
        "\n",
        "    print(\"person \", i, \" : \", emotion_dict[maxindex], \"-->\", emojis[maxindex])\n",
        "    cv2.putText(image, emotion_dict[maxindex], (x+10, y-20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "    i += 1\n",
        "\n",
        "  cv2_imshow(image)"
      ],
      "metadata": {
        "id": "OZsnBdQe6kN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXxZnpF1BliC"
      },
      "source": [
        "_predict(\"khans.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyxz0H92l7WR"
      },
      "source": [
        "_predict(\"Ram_Charan.webp\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuVB1zlnpspz"
      },
      "source": [
        "_predict(\"mb.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxz-mznjqEYj"
      },
      "source": [
        "_predict(\"me3.PNG\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D-htgZ4v1df"
      },
      "source": [
        "_predict(\"ntr.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6BEadDySv3v"
      },
      "source": [
        "_predict(\"images\\me3.PNG\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpQDSjHYsa6Q"
      },
      "source": [
        "**Intuition or insights**\n",
        "---\n",
        "* cv2.imshow()\n",
        " is incampatible with google colabs ..we use cv2_imshow() from google.colab.patches\n",
        "* Ipython . Image can also be used to display images in google colabs\n",
        "* we can preprocess the testing image using opencv\n",
        "  * opencv() has so many functionalities\n",
        "    1. imread() --> img path\n",
        "    2. cvtColor() --> img , cv2.COLOR_BGR2GRAY\n",
        "    3. cascadeClassifier() --> detectMultiScale() --> to detect multiple faces in image\n",
        "    4. rectangle() --> img, startpoint, endpoint, color,thickness\n",
        "    5. putText() -->img, text, coordinates, font, fontScale, color, thickness, lineType\n",
        "\n",
        "---\n",
        "* np.argmax(predictions) ==> to get the maximum confidence prediction\n",
        "* croping of image --> use the power of slicing\n",
        "* cascade classifier ==> ensemble learning based on concatenation of several classifiers\n",
        "    * not multiexpert --> multistage\n",
        "    * combinatorial nature of the classification\n",
        "* haar cascade classifier ==> highly pretrained models stored in xml files\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nuLnxqYq-E6"
      },
      "source": [
        "**Error log :**\n",
        "\n",
        "1. opencv .. !_src.empty() in function \"cvtColor\" ===> it happends if there is a wrong in previous stmt --> .imread()\n",
        "2. opencv .. !empty() in function detectMultiScale ===> xml file is missing\n",
        "3. cannot create group in read only mode ===> model is not defined but loaded\n",
        "4. model is not defined ===> define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZ16t7K8iRCS"
      },
      "source": [
        "\"\"\" code snippet to display image \"\"\"\n",
        "\n",
        "from IPython.display import Image\n",
        "Image('/content/me.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNB2QgwJLA-j"
      },
      "source": [
        "* **saving the weights of the model and again loading it in keras**\n",
        "* **saving the architecture of the model in json file using model_from_json**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pgRxjAyjzq0"
      },
      "source": [
        "model.save_weights(\"model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lD0bddwj1Sx"
      },
      "source": [
        "model.load_weights(\"model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N600QexbpCJd"
      },
      "source": [
        "from keras.models import model_from_json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6mmEsNVpCMv"
      },
      "source": [
        "model_json = model.to_json()\n",
        "with open(\"model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "#model.save_weights(\"model.h5\")\n",
        "print(\"Saved model to disk\")\n",
        "\n",
        "# later...\n",
        "\n",
        "# load json and create model\n",
        "json_file = open('model.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "model = model_from_json(loaded_model_json)\n",
        "# load weights into new model\n",
        "model.load_weights(\"model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJgyEXKRogSZ"
      },
      "source": [
        "\"\"\" code snippet for Downloading files from colabs \"\"\"\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"model.h5\")\n",
        "files.download(\"model.json\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D_G0SuqOeA1"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJxyoe0SpC2k"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s38ekv_fqSwP"
      },
      "source": [
        "\"\"\" loading the model in modular approach \"\"\"\n",
        "\n",
        "def load_model_():\n",
        "  json_file = open('model.json', 'r')\n",
        "  loaded_model_json = json_file.read()\n",
        "  json_file.close()\n",
        "  model = model_from_json(loaded_model_json)\n",
        "  # load weights into new model\n",
        "  model.load_weights(\"model.h5\")\n",
        "  return model\n",
        "\n",
        "\n",
        "model = load_model_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fM4bcS9EqKNt"
      },
      "source": [
        "model2 = load_model_()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}