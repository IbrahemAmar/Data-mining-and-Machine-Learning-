{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IbrahemAmar/Data-mining-and-Machine-Learning-/blob/main/lab3_ex1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import regular expressins packge\n",
        "# import numbers package\n",
        "import numpy as np\n",
        "import re"
      ],
      "metadata": {
        "id": "JZ-BUBW7n6xa"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#read file\n",
        "def readFile(fileName):\n",
        "    file = open(fileName,'r',encoding=\"cp437\")\n",
        "    fileText = \"\"\n",
        "    for line in file:\n",
        "        fileText += line\n",
        "    return fileText"
      ],
      "metadata": {
        "id": "gWMpvhX6n6uu"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess text\n",
        "def preProcess(text):\n",
        "    # Remove non-letter chars\n",
        "    text = re.sub(\"[^a-zA-Z ]\",\" \", text)\n",
        "    # Change characters to lower\n",
        "    text = text.lower()\n",
        "    return text"
      ],
      "metadata": {
        "id": "r-RU0VXCn6sB"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def genReferenceWordList(texts,stopWords):\n",
        "# concatenate the texts\n",
        "    allText = \"\"\n",
        "    for line in texts:\n",
        "        allText += line\n",
        "    # Generate a word list\n",
        "    wordsList =  allText.split()\n",
        "    # Generate a word set\n",
        "    wordsSet =  set(wordsList)\n",
        "    # Remove the stop words from the word list\n",
        "    stopWordsList = stopWords.split()\n",
        "    stopWordsSet = set(stopWordsList)\n",
        "    refWordSet = wordsSet.difference(stopWordsSet)\n",
        "    return list(refWordSet)"
      ],
      "metadata": {
        "id": "Z4jTb5lMn6pc"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vector distance\n",
        "# find the distance between arrays\n",
        "def vecDist(vec1,vec2):\n",
        "    vecDiff = vec1-vec2\n",
        "    # compute the distance (\"pitagoras\")\n",
        "    vecSqr =  np.square(vecDiff)\n",
        "    vecSum =  np.sum(vecSqr)\n",
        "    return np.sqrt(vecSum)"
      ],
      "metadata": {
        "id": "QPAcaH7qn6mw"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word frequency\n",
        "def getWordFrequency(texts,dictList):\n",
        "    dictSize = len(dictList)\n",
        "    nTexts = len(texts)\n",
        "    wordFreq = np.empty((nTexts,dictSize))\n",
        "    for i in range(nTexts):\n",
        "        print(\"text\" + str(i))\n",
        "        for j in range(dictSize):\n",
        "            wordFreq[i,j] = texts[i].count(dictList[j])\n",
        "    return wordFreq"
      ],
      "metadata": {
        "id": "7a56UQcNoQ-q"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "books = []\n",
        "#read  and preprocess files\n",
        "books+= [readFile('DB.txt')]\n",
        "books+= [readFile('Eliot.txt')]\n",
        "books+= [readFile('HP.txt')]\n",
        "books+= [readFile('Tolkien.txt')]\n",
        "books+= [readFile('xxx.txt')]"
      ],
      "metadata": {
        "id": "-Dm1gm3OoQ8F"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window = 150000"
      ],
      "metadata": {
        "id": "AGuA1zpvoQ5Z"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split every book into two parts\n",
        "bookParts = []\n",
        "for book in books:\n",
        "    nParts = len(book)//window\n",
        "    for i in range(nParts):\n",
        "        bookParts += [book[i*window:(i+1)*window]]"
      ],
      "metadata": {
        "id": "t9SSIjAloU7G"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess texts in book parts\n",
        "texts = []\n",
        "for text in bookParts:\n",
        "\n",
        "    # 1. your existing preprocess\n",
        "    p = preProcess(text)\n",
        "\n",
        "    # 6. remove words of length 1 or 2 (using regex)\n",
        "    # \\b : word boundary\n",
        "    # [a-zA-Z]{1,2} : any 1 or 2 letter word\n",
        "    # \\s+ : optional whitespace cleanup\n",
        "    p = re.sub(r'\\b[a-zA-Z]{1,2}\\b', '', p)\n",
        "\n",
        "    # clean double spaces caused by removals\n",
        "    p = re.sub(r'\\s+', ' ', p).strip()\n",
        "\n",
        "    texts.append(p)\n"
      ],
      "metadata": {
        "id": "P2Fqrop4oUyi"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read stop words file - words that can be removed\n",
        "stopWords = readFile('stopwords_en.txt')"
      ],
      "metadata": {
        "id": "Mupvf2ujoYLz"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate reference word list\n",
        "refList = genReferenceWordList(texts,stopWords)"
      ],
      "metadata": {
        "id": "9SrXG7gRoYDe"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the frequency of the reference words in the files\n",
        "wordFreq = getWordFrequency(texts,refList)"
      ],
      "metadata": {
        "id": "jWIATzKhobfz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1060470e-36db-46eb-b8f3-33c2db1f9529"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text0\n",
            "text1\n",
            "text2\n",
            "text3\n",
            "text4\n",
            "text5\n",
            "text6\n",
            "text7\n",
            "text8\n",
            "text9\n",
            "text10\n",
            "text11\n",
            "text12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "-OBJg5O_n3gj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "135f79ae-47e7-43cd-fcf5-6ff9fedba1cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dist matrix = \n",
            " [[   0.  611.  703. 1098. 1024. 1025. 1343. 1309. 1264. 1130. 1221. 1197.\n",
            "  1296.]\n",
            " [ 611.    0.  534. 1093. 1000. 1000. 1310. 1289. 1236. 1097. 1180. 1158.\n",
            "  1269.]\n",
            " [ 703.  534.    0. 1097. 1006. 1014. 1355. 1327. 1272. 1124. 1222. 1204.\n",
            "  1305.]\n",
            " [1098. 1093. 1097.    0.  520.  519. 1273. 1211. 1175.  949. 1054. 1034.\n",
            "  1187.]\n",
            " [1024. 1000. 1006.  520.    0.  477. 1210. 1167. 1110.  893. 1023.  995.\n",
            "  1134.]\n",
            " [1025. 1000. 1014.  519.  477.    0. 1207. 1155. 1103.  889.  997.  960.\n",
            "  1123.]\n",
            " [1343. 1310. 1355. 1273. 1210. 1207.    0.  592.  646. 1297. 1298. 1260.\n",
            "   568.]\n",
            " [1309. 1289. 1327. 1211. 1167. 1155.  592.    0.  652. 1252. 1272. 1246.\n",
            "   419.]\n",
            " [1264. 1236. 1272. 1175. 1110. 1103.  646.  652.    0. 1190. 1212. 1176.\n",
            "   380.]\n",
            " [1130. 1097. 1124.  949.  893.  889. 1297. 1252. 1190.    0.  677.  667.\n",
            "  1226.]\n",
            " [1221. 1180. 1222. 1054. 1023.  997. 1298. 1272. 1212.  677.    0.  530.\n",
            "  1253.]\n",
            " [1197. 1158. 1204. 1034.  995.  960. 1260. 1246. 1176.  667.  530.    0.\n",
            "  1219.]\n",
            " [1296. 1269. 1305. 1187. 1134. 1123.  568.  419.  380. 1226. 1253. 1219.\n",
            "     0.]]\n"
          ]
        }
      ],
      "source": [
        "# find the distance matrix between the text files\n",
        "# text distances\n",
        "rows,colomns = wordFreq.shape\n",
        "\n",
        "dist = np.zeros((rows,rows))\n",
        "for i in range(rows):\n",
        "    for j in range(rows):\n",
        "        # calculate the distance between the frequency vectors\n",
        "        dist[i,j] = round(vecDist(wordFreq[i],wordFreq[j]))\n",
        "\n",
        "print(\"dist matrix = \\n\",dist)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(books[-3])//window   #HP book"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6k2QrSpopFv",
        "outputId": "fb1b31e0-22ab-4fdf-ae30-72d50489e252"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    }
  ]
}